{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96b90e1-3c43-455b-8c95-e2f9e7f43930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import IPython.display as ipd\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from transformers import AutoTokenizer, AddedToken\n",
    "from streaming import MDSWriter\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "from streaming import LocalDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocess import Pool\n",
    "import itertools\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield (l[i: i + n], i // n)\n",
    "\n",
    "def multiprocessing(strings, function, cores=6, returned=True):\n",
    "    df_split = chunks(strings, len(strings) // cores)\n",
    "    pool = Pool(cores)\n",
    "    pooled = pool.map(function, df_split)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    if returned:\n",
    "        return list(itertools.chain(*pooled))\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "columns = {\n",
    "    'input_ids': 'uint32',\n",
    "    'position_ids': 'uint32',\n",
    "    'attention_mask': 'uint32',\n",
    "    'audio': 'str',\n",
    "    'text': 'str'\n",
    "}\n",
    "hashes = 'sha1', 'xxh64'\n",
    "\n",
    "def new_path(f):\n",
    "    splitted = f.split('/')\n",
    "    base_folder = splitted[0] + '_trim'\n",
    "    splitted = '/'.join([base_folder] + splitted[1:])\n",
    "    return splitted\n",
    "\n",
    "def new_path_neucodec(f):\n",
    "    splitted = f.split('/')\n",
    "    folder = f.split('/')[0]\n",
    "    folder = folder + '_neucodec'\n",
    "    new_f = os.path.join(folder, '/'.join(splitted[1:]))\n",
    "    new_f = new_f.replace('.mp3', '.json').replace('.wav', '.json')\n",
    "    return new_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e1c342e-de1c-4b68-93dd-7123b417be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Scicom-intl/Malaysian-Chinese-Emilia\", \"speaker_permutation_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daddd0d7-ddde-4c5c-9171-a6de35a91001",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject = load_dataset(\"Scicom-intl/Malaysian-Chinese-Emilia\", \"audio_length_ratio_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f9088f-64de-4346-b990-5518fd2fe92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 605169/605169 [00:22<00:00, 26471.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30887"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reject_audio = set()\n",
    "\n",
    "for i in tqdm(range(len(reject['train']))):\n",
    "    if not reject['train'][i]['audio_length_ratio_text_accept']:\n",
    "        reject_audio.add(reject['train'][i]['audio_filename'])\n",
    "\n",
    "len(reject_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93ac400-d1f8-49b0-b62f-787a2c190915",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = ds['train'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "241f03e6-e461-4422-a27b-288712f93d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = new_path_neucodec(new_path(rows[0]['reference_audio']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31bf0c93-e958-40f7-b342-821e489a2dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65537"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-1.7B-Base')\n",
    "extra = [AddedToken('<|speech_start|>')]\n",
    "for i in range(65536):\n",
    "    extra.append(AddedToken(f'<|s_{i}|>'))\n",
    "tokenizer.add_tokens(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8ea63ad-bc36-485e-8d3f-66ad56f9502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def collator(batch, batch_position_ids):\n",
    "    input_ids = []\n",
    "    position_ids = []\n",
    "    masks = []\n",
    "    for i in range(len(batch)):\n",
    "        l = len(batch[i])\n",
    "        input_ids.extend(batch[i])\n",
    "        position_ids.extend(batch_position_ids[i])\n",
    "        masks.append(l)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': np.array(input_ids).astype(np.uint32),\n",
    "        'position_ids': np.array(position_ids).astype(np.uint32),\n",
    "        'attention_mask': np.array(masks).astype(np.uint32),\n",
    "        'audio': '',\n",
    "        'text': '',\n",
    "    }\n",
    "\n",
    "def slice_and_balance(nested_list, size):\n",
    "    first = []\n",
    "    balance = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sublist in nested_list:\n",
    "        if current_size < size:\n",
    "            remaining_space = size - current_size\n",
    "            if len(sublist) <= remaining_space:\n",
    "                first.append(sublist)\n",
    "                current_size += len(sublist)\n",
    "            else:\n",
    "                first.append(sublist[:remaining_space])\n",
    "                balance.append(sublist[remaining_space:])\n",
    "                current_size = size\n",
    "        else:\n",
    "            balance.append(sublist)\n",
    "    \n",
    "    return first, balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b488f43-60a6-4353-8f37-01fd89cd8e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sequence_length = 1024 * 10\n",
    "def loop(files, block_size = sequence_length):\n",
    "    rows, index = files\n",
    "    out_root = f'gfs/01be5b33/malaysian-mandarin-emilia/tokenized-{index}'\n",
    "    os.system(f'rm -rf {out_root}')\n",
    "    count = 0\n",
    "    temp = []\n",
    "    position_ids = []\n",
    "    last_block, last_position_block = None, None\n",
    "    with MDSWriter(out=out_root, columns=columns, compression=None, hashes=hashes) as out:\n",
    "        for row in tqdm(rows):\n",
    "\n",
    "            if row['reference_audio'] in reject_audio:\n",
    "                continue\n",
    "\n",
    "            if row['target_audio'] in reject_audio:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(new_path_neucodec(new_path(row['reference_audio']))) as fopen:\n",
    "                    left = json.load(fopen)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                with open(new_path_neucodec(new_path(row['target_audio']))) as fopen:\n",
    "                    right = json.load(fopen)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            left_text = row['reference_text'].strip()\n",
    "            right_text = row['target_text'].strip()\n",
    "\n",
    "            if len(left_text.split()) > len(left):\n",
    "                continue\n",
    "\n",
    "            if len(right_text.split()) > len(right):\n",
    "                continue\n",
    "            \n",
    "            left_token = ''.join([f'<|s_{t}|>' for t in left])\n",
    "            right_token = ''.join([f'<|s_{t}|>' for t in right])\n",
    "            \n",
    "            left_prompt = f'<|im_start|>{left_text}<|speech_start|>{left_token}<|im_end|>'\n",
    "            right_prompt = f'<|im_start|>{right_text}<|speech_start|>{right_token}<|im_end|>'\n",
    "\n",
    "            prompt = left_prompt + right_prompt\n",
    "            \n",
    "            outputs = tokenizer(prompt, add_special_tokens = False)\n",
    "            position = range(len(outputs['input_ids']))\n",
    "            length = len(outputs['input_ids'])\n",
    "            \n",
    "            if count + length > block_size:\n",
    "                o = collator(temp, position_ids)\n",
    "                if o['input_ids'].shape[0] > 0:\n",
    "                    out.write(o)\n",
    "                temp = [outputs['input_ids']]\n",
    "                position_ids = [position]\n",
    "                count = length\n",
    "                \n",
    "            else:\n",
    "                temp.append(outputs['input_ids'])\n",
    "                position_ids.append(range(len(outputs['input_ids'])))\n",
    "                count += len(outputs['input_ids'])\n",
    "        \n",
    "        if len(temp):\n",
    "            o = collator(temp, position_ids)\n",
    "            if o['input_ids'].shape[0] > 0:\n",
    "                out.write(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8034e74-f9fe-43da-bbdb-0a1a04378527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multiprocessing(rows, loop, cores = 40, returned = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e5563c5-69f6-410e-a91c-80e54c8cdbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = sorted(glob('gfs/01be5b33/malaysian-mandarin-emilia/tokenized-*'), key = lambda x: int(x.split('-')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e71d7ce7-457c-44c4-bd28-732cfb9e05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf gfs/01be5b33/multipacking-malaysian-mandarin-emilia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "312004fb-67b2-4840-aa6e-491ecd3b9a78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with MDSWriter(out='gfs/01be5b33/multipacking-malaysian-mandarin-emilia', columns=columns, compression=None, hashes=hashes) as out:\n",
    "    for f in folders:\n",
    "        try:\n",
    "            dataset = LocalDataset(local=f)\n",
    "            for i in tqdm(range(len(dataset))):\n",
    "                out.write(dataset[i])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f75f5b6c-1edf-46e9-a95d-fd32a692fb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178244"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = LocalDataset('gfs/01be5b33/multipacking-malaysian-mandarin-emilia')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df3c9a99-e423-42e6-aed7-6c3711545546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.82521856"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(dataset) * 10240) / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caa2543a-c0f7-4aa9-844b-0d4f5768aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hf upload Scicom-intl/Malaysian-Chinese-Emilia-multipacking-10k \\\n",
    "# gfs/01be5b33/multipacking-malaysian-mandarin-emilia \\\n",
    "# --repo-type=dataset --private"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d502557-0fe4-45ec-9f65-791658b3beab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malaysian-reasoning",
   "language": "python",
   "name": "malaysian-reasoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
